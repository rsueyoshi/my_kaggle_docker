dataset: 
    json_filepath: /kaggle/input/pii-detection-removal-from-educational-data/train.json
    extra_filepath:
    - /kaggle/input/pii-mixtral8x7b-generated-essays/mpware_mixtral8x7b_v1.1-no-i-username.json
fold:
    num_folds: 4
    fold: 0
model_class: DebertaV2ForTokenClassification
architecture:
    backbone: microsoft/deberta-v3-large
    name: ec2_deberta_large_fp16_0966_return_overflowing_tokens
    fleeze_layers: 6
environment:
    mixed_precision: true
    number_of_workers: 4
    seed: 42
experiment_name: efficiency_model
tokenizer:
    max_length: 1800
    stride: 128
training:
    batch_size: 1
    eval_batch_size: 1
    epochs: 3
    grad_accumulation: 16
    learning_rate: 2.5e-5
    schedule: linear 
    evaluation_strategy: steps
    eval_steps: 50
    metric_for_best_model: f5
    warmup_ratio: 0.1
    weight_decay: 0.01
    bf16: False
    fp16: true
output:
    suffix: /kaggle/output/deberta3large
debug: False